# app.py
import os
from operator import itemgetter

import streamlit as st
from dotenv import load_dotenv

from langchain_community.chat_models import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel
from langchain_chroma.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Env & Embeddings
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()  # OPENAI_API_KEY í•„ìš” (OpenAIEmbeddings ì‚¬ìš© ì‹œ)
emb = OpenAIEmbeddings(model="text-embedding-3-small")
PERSIST_DIR = "./db"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utils
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

def last_k_turns(history, k=6):
    """Human/AI ë©”ì‹œì§€ ê¸°ì¤€ ìµœê·¼ kê°œë§Œ ë‚¨ê²¨ í† í° ì ˆì•½."""
    turns = []
    cnt = 0
    for m in reversed(history):
        if isinstance(m, (HumanMessage, AIMessage)):
            turns.append(m)
            cnt += 1
            if cnt >= k:
                break
    return list(reversed(turns))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompts (ë°©ì‹ A: ì§ˆë¬¸ ì •ì œ â†’ ê²€ìƒ‰ â†’ ë‹µë³€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ 'ë…ë¦½ ì§ˆë¬¸'ìœ¼ë¡œ ì¬ì‘ì„±
CONDENSE_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "You rewrite the user's latest question to be self-contained using the chat history as hints. "
     "Return only the rewritten question text."),
    MessagesPlaceholder("chat_history"),
    ("human", "Latest user question: {question}")
])

# 2) ìµœì¢… ë‹µë³€: (íˆìŠ¤í† ë¦¬ + ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸) ì œê³µ
ANSWER_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "You are a helpful assistant. Use only the given CONTEXT to answer the QUESTION. "
     "If the context is insufficient, say you don't know in Korean."),
    MessagesPlaceholder("chat_history"),
    ("human",
     "CONTEXT:\n{context}\n\nQUESTION:\n{question}\n\nAnswer in Korean, be concise.")
])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Chain builder (cached)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def build_chain(option_name):
    # 1) ê¸°ì¡´ Chroma DB ë³µêµ¬
    if option_name == "ì²­ë…„ë§¤ì…ì„ëŒ€":
        vector_store = Chroma(
            persist_directory=PERSIST_DIR,
            embedding_function=emb,
            collection_name="YOUTH" 
        )
        gu_li = ['ê°•ë‚¨êµ¬','ê°•ë™êµ¬','ê°•ë¶êµ¬','ê°•ì„œêµ¬','ê´€ì•…êµ¬','ê´‘ì§„êµ¬','êµ¬ë¡œêµ¬','ê¸ˆì²œêµ¬','ë…¸ì›êµ¬','ë„ë´‰êµ¬','ë™ëŒ€ë¬¸êµ¬','ë§ˆí¬êµ¬','ì„œëŒ€ë¬¸êµ¬','ì„œì´ˆêµ¬','ì„±ë¶êµ¬','ì†¡íŒŒêµ¬','ì–‘ì²œêµ¬','ì˜ë“±í¬êµ¬','ìš©ì‚°êµ¬','ì€í‰êµ¬','ì¢…ë¡œêµ¬','ì¤‘êµ¬','ì¤‘ë‘êµ¬']

    elif option_name == "ì‹ í˜¼Â·ì‹ ìƒì•„ë§¤ì…ì„ëŒ€â… ":
        vector_store = Chroma(
            persist_directory=PERSIST_DIR,
            embedding_function=emb,
            collection_name="MARRY1"
        )
        gu_li = ['ê°•ë‚¨êµ¬','ê°•ë™êµ¬','ê°•ë¶êµ¬','ê°•ì„œêµ¬','ê´‘ì§„êµ¬','ë…¸ì›êµ¬','ë„ë´‰êµ¬','ë™ëŒ€ë¬¸êµ¬','ë™ì‘êµ¬','ë§ˆí¬êµ¬','ì„œëŒ€ë¬¸êµ¬','ì„œì´ˆêµ¬','ì„±ë¶êµ¬','ì†¡íŒŒêµ¬','ì–‘ì²œêµ¬','ì˜ë“±í¬êµ¬','ì€í‰êµ¬','ì¤‘ë‘êµ¬']

    elif option_name ==  "ì‹ í˜¼Â·ì‹ ìƒì•„_ë§¤ì…ì„ëŒ€ì£¼íƒâ…¡":
        vector_store = Chroma(
            persist_directory=PERSIST_DIR,
            embedding_function=emb,
            collection_name="MARRY2"
        )
        gu_li = ['ê°•ë™êµ¬','ê°•ë¶êµ¬','ê°•ì„œêµ¬','ê´‘ì§„êµ¬','êµ¬ë¡œêµ¬','ê¸ˆì²œêµ¬','ë…¸ì›êµ¬','ë„ë´‰êµ¬','ë™ëŒ€ë¬¸êµ¬','ë™ì‘êµ¬','ì„œëŒ€ë¬¸êµ¬','ì„œì´ˆêµ¬','ì„±ë¶êµ¬','ì†¡íŒŒêµ¬','ì–‘ì²œêµ¬','ì˜ë“±í¬êµ¬','ì€í‰êµ¬','ì¤‘ë‘êµ¬']
    retriever = vector_store.as_retriever(search_kwargs={"k": 4})

    # 2) LLM (Ollama)
    llm = ChatOllama(model="eeve-q5km:latest", temperature=0.2, streaming=True)

    # 3) ë…ë¦½ ì§ˆë¬¸ ìƒì„±ê¸° (ì…ë ¥: {"question": str, "chat_history": [BaseMessage,...]} â†’ ì¶œë ¥: str)
    make_standalone = CONDENSE_PROMPT | llm | StrOutputParser()

    # 4) íŒŒì´í”„ë¼ì¸
    #    ë‹¨ê³„A: standalone_question ìƒì„± + chat_history ë³´ì¡´  â†’ í‚¤ í™•ì • (RunnableParallelë¡œ ì•ˆì „í•˜ê²Œ)
    stepA = RunnableParallel(
        standalone_question={
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        } | make_standalone,
        chat_history=itemgetter("chat_history"),
    )

    #    ë‹¨ê³„B: ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸/ì§ˆë¬¸/íˆìŠ¤í† ë¦¬ ë¬¶ê¸° â†’ ANSWER_PROMPT
    stepB = RunnableParallel(
        context=itemgetter("standalone_question") | retriever | format_docs,
        question=itemgetter("standalone_question"),
        chat_history=itemgetter("chat_history"),
    )

    # 5) ì „ì²´ RAG ì²´ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì‹œ ë¬¸ìì—´ chunkë¥¼ ë°˜í™˜)
    rag_chain = stepA | stepB | ANSWER_PROMPT | llm | StrOutputParser()
    return rag_chain, gu_li, vector_store

def set_filter(vector_store, filter):
    retriever = vector_store.as_retriever(
    search_kwargs={"k": 10, "filter": filter}
    )
    # 2) LLM (Ollama)
    llm = ChatOllama(model="eeve-q5km:latest", temperature=0.2, streaming=True)

    # 3) ë…ë¦½ ì§ˆë¬¸ ìƒì„±ê¸° (ì…ë ¥: {"question": str, "chat_history": [BaseMessage,...]} â†’ ì¶œë ¥: str)
    make_standalone = CONDENSE_PROMPT | llm | StrOutputParser()

    # 4) íŒŒì´í”„ë¼ì¸
    #    ë‹¨ê³„A: standalone_question ìƒì„± + chat_history ë³´ì¡´  â†’ í‚¤ í™•ì • (RunnableParallelë¡œ ì•ˆì „í•˜ê²Œ)
    stepA = RunnableParallel(
        standalone_question={
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        } | make_standalone,
        chat_history=itemgetter("chat_history"),
    )

    #    ë‹¨ê³„B: ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸/ì§ˆë¬¸/íˆìŠ¤í† ë¦¬ ë¬¶ê¸° â†’ ANSWER_PROMPT
    stepB = RunnableParallel(
        context=itemgetter("standalone_question") | retriever | format_docs,
        question=itemgetter("standalone_question"),
        chat_history=itemgetter("chat_history"),
    )

    # 5) ì „ì²´ RAG ì²´ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì‹œ ë¬¸ìì—´ chunkë¥¼ ë°˜í™˜)
    rag_chain = stepA | stepB | ANSWER_PROMPT | llm | StrOutputParser()

    return rag_chain

def reset_ragchain(vector_store):
    retriever = vector_store.as_retriever(search_kwargs={"k": 4})

    # 2) LLM (Ollama)
    llm = ChatOllama(model="eeve-q5km:latest", temperature=0.2, streaming=True)

    # 3) ë…ë¦½ ì§ˆë¬¸ ìƒì„±ê¸° (ì…ë ¥: {"question": str, "chat_history": [BaseMessage,...]} â†’ ì¶œë ¥: str)
    make_standalone = CONDENSE_PROMPT | llm | StrOutputParser()

    # 4) íŒŒì´í”„ë¼ì¸
    #    ë‹¨ê³„A: standalone_question ìƒì„± + chat_history ë³´ì¡´  â†’ í‚¤ í™•ì • (RunnableParallelë¡œ ì•ˆì „í•˜ê²Œ)
    stepA = RunnableParallel(
        standalone_question={
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        } | make_standalone,
        chat_history=itemgetter("chat_history"),
    )

    #    ë‹¨ê³„B: ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸/ì§ˆë¬¸/íˆìŠ¤í† ë¦¬ ë¬¶ê¸° â†’ ANSWER_PROMPT
    stepB = RunnableParallel(
        context=itemgetter("standalone_question") | retriever | format_docs,
        question=itemgetter("standalone_question"),
        chat_history=itemgetter("chat_history"),
    )

    # 5) ì „ì²´ RAG ì²´ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì‹œ ë¬¸ìì—´ chunkë¥¼ ë°˜í™˜)
    rag_chain = stepA | stepB | ANSWER_PROMPT | llm | StrOutputParser()

    return rag_chain
    

def reset_session_and_cache():
    # ì±„íŒ… ì´ë ¥ ì´ˆê¸°í™”
    st.session_state["lc_messages"] = [AIMessage(content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")]
    # ì²´ì¸ ìºì‹œ ë¬´íš¨í™” í›„ ì¦‰ì‹œ ë¦¬ëŸ°
    build_chain.clear()
    st.rerun()

with st.sidebar:
    st.header("âš™ï¸ ìœ í˜• ì„¤ì •")
    option_name = st.selectbox(
        "ìœ í˜•",
        options=["ì²­ë…„ë§¤ì…ì„ëŒ€", "ì‹ í˜¼Â·ì‹ ìƒì•„ë§¤ì…ì„ëŒ€â… ", "ì‹ í˜¼Â·ì‹ ìƒì•„_ë§¤ì…ì„ëŒ€ì£¼íƒâ…¡"],
        index=0,
        key="option_name",
        on_change=reset_session_and_cache
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ğŸ’¬Chat bot")
rag_chain, gu_li, vector_store = build_chain(option_name)

st.write(vector_store._collection_name)
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (UIì—” ê°„ë‹¨ ì¸ì‚¬ë§Œ í‘œì‹œ)
if "lc_messages" not in st.session_state:
    st.session_state.lc_messages = [AIMessage(content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")]

# ëŒ€í™” ë Œë”ë§ (SystemMessageëŠ” í™”ë©´ì—ì„œ ìƒëµ)
for m in st.session_state.lc_messages:
    if isinstance(m, SystemMessage):
        continue
    role = "assistant" if isinstance(m, AIMessage) else "user"
    with st.chat_message(role):
        st.markdown(m.content)

# ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :")
if user_input:
    for gu in gu_li:
        if gu in user_input:
            filters = {"êµ¬": gu} 
            rag_chain = set_filter(vector_store, filters)
        else:
            rag_chain = reset_ragchain(vector_store)
           
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ ì´ë ¥ì— ì¶”ê°€ & ë Œë”
    st.session_state.lc_messages.append(HumanMessage(content=user_input))
    with st.chat_message("user"):
        st.markdown(user_input)

    # 2) íˆìŠ¤í† ë¦¬ ì¤€ë¹„(ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸, ìµœê·¼ kí„´ë§Œ ìœ ì§€)
    chat_history = [m for m in st.session_state.lc_messages if not isinstance(m, SystemMessage)]
    chat_history = last_k_turns(chat_history, k=6)

    # 3) ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    with st.chat_message("assistant"):
        box = st.empty()
        buf = ""
        with st.spinner("Chat botì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ..."):
            # ì…ë ¥ì€ í•­ìƒ {"question": <str>, "chat_history": <list>} í˜•íƒœ
            for chunk in rag_chain.stream({
                "question": user_input,
                "chat_history": chat_history,
                "k": 10
            }):
                buf += chunk              # StrOutputParser ì‚¬ìš© â†’ chunkëŠ” ë¬¸ìì—´
                box.markdown(buf)

    # 4) ìµœì¢… ë‹µë³€ ì´ë ¥ì— ì¶”ê°€
    st.session_state.lc_messages.append(AIMessage(content=buf))

