{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac2674e-a87b-49b9-a3d3-6c4dac28ca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.43.0\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets>=2.19.0\n",
      "  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting accelerate>=0.33.0\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl>=0.9.6\n",
      "  Using cached trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft>=0.11.1\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bitsandbytes>=0.43.1\n",
      "  Using cached bitsandbytes-0.47.0-py3-none-win_amd64.whl.metadata (11 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: python-docx in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\venus\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=4.43.0) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from transformers>=4.43.0) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\venus\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.43.0) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.43.0) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from datasets>=2.19.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from datasets>=2.19.0) (0.4.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from datasets>=2.19.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from datasets>=2.19.0) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (3.12.15)\n",
      "Requirement already satisfied: psutil in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from accelerate>=0.33.0) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from accelerate>=0.33.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: networkx in c:\\users\\venus\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.0.0->accelerate>=0.33.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.33.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.33.0) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.33.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\venus\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.33.0) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from python-docx) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\venus\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=2.19.0) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from requests->transformers>=4.43.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from requests->transformers>=4.43.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from requests->transformers>=4.43.0) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\venus\\anaconda3\\envs\\3rd_project\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.33.0) (3.0.2)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Using cached trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached bitsandbytes-0.47.0-py3-none-win_amd64.whl (60.7 MB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 25.6 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece, bitsandbytes, accelerate, transformers, datasets, trl, peft\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [bitsandbytes]\n",
      "   ----- ---------------------------------- 1/7 [bitsandbytes]\n",
      "   ----- ---------------------------------- 1/7 [bitsandbytes]\n",
      "   ----- ---------------------------------- 1/7 [bitsandbytes]\n",
      "   ----- ---------------------------------- 1/7 [bitsandbytes]\n",
      "   ----------- ---------------------------- 2/7 [accelerate]\n",
      "   ----------- ---------------------------- 2/7 [accelerate]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ----------------- ---------------------- 3/7 [transformers]\n",
      "   ---------------------- ----------------- 4/7 [datasets]\n",
      "   ---------------------- ----------------- 4/7 [datasets]\n",
      "   ---------------------- ----------------- 4/7 [datasets]\n",
      "   ---------------------------- ----------- 5/7 [trl]\n",
      "   ---------------------------- ----------- 5/7 [trl]\n",
      "   ---------------------------- ----------- 5/7 [trl]\n",
      "   ---------------------------- ----------- 5/7 [trl]\n",
      "   ---------------------------------- ----- 6/7 [peft]\n",
      "   ---------------------------------- ----- 6/7 [peft]\n",
      "   ---------------------------------- ----- 6/7 [peft]\n",
      "   ---------------------------------------- 7/7 [peft]\n",
      "\n",
      "Successfully installed accelerate-1.10.1 bitsandbytes-0.47.0 datasets-4.1.1 peft-0.17.1 sentencepiece-0.2.1 transformers-4.56.2 trl-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"transformers>=4.43.0\" \"datasets>=2.19.0\" \"accelerate>=0.33.0\" \\\n",
    "\"trl>=0.9.6\" \"peft>=0.11.1\" \"bitsandbytes>=0.43.1\" \\\n",
    "\"sentencepiece\" \"pandas\" \"python-docx\" \"tqdm\" \"scikit-learn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515ffd67-fcdd-49e4-b3a1-5b8835d8bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing: ['huggingface_hub>=0.24.6', 'safetensors']\n",
      "Requirement already satisfied: huggingface_hub>=0.24.6 in /usr/local/lib/python3.11/dist-packages (0.35.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.24.6) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.24.6) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.24.6) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.24.6) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.24.6) (2025.1.31)\n",
      "huggingface_hub: 0.35.1\n",
      "safetensors    : 0.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 주피터 커널에 설치 (런타임 재시작 불필요)\n",
    "import sys, subprocess\n",
    "def pip_install(pkgs):\n",
    "    print(\"Installing:\", pkgs)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\"] + pkgs)\n",
    "\n",
    "pip_install([\"huggingface_hub>=0.24.6\", \"safetensors\"])\n",
    "# (옵션) 없으면 같이 깔기\n",
    "# pip_install([\"transformers>=4.43.0\"])\n",
    "# pip_install([\"tqdm\"])\n",
    "import huggingface_hub, safetensors\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"safetensors    :\", safetensors.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e321b6-c3fc-4932-87b5-0d31f296aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Download start ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17272ec3c2084d5a9882ac2b1cbaa2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22cd323a25c44779b15c255c993be31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00009.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fa7488c3a548c5a0471c0aa8e33a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00009.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4520877f9c417aa977eb191973a1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00009.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cb7c6dceba4289961a77ece5e8572e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00009.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f8ba66064b42d591fe7541d85481db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00009.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb3815fc4684c14ba8762ce533095d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00009.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f63dfafbda4f4799472f47041fd595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00009.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedb251f8cfd4d7c94753c322bfdf6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00009.safetensors:   0%|          | 0.00/4.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Download done to: /workspace/solar_model/model\n",
      "\n",
      "== Verify ==\n",
      "총 샤드: 9\n",
      "누락 샤드: []\n",
      "의심(너무 작은) 샤드 수: 0\n",
      "\n",
      "== Disk ==\n",
      "Filesystem                    Size  Used Avail Use% Mounted on\n",
      "mfs#ca-mtl-3.runpod.net:9421  420T  329T   91T  79% /workspace\n",
      "\n",
      "✅ 모든 샤드/파일 정상 다운로드로 보입니다.\n"
     ]
    }
   ],
   "source": [
    "# === One-Shot: 환경 세팅 → 안전 다운로드(순차/재개) → 검증 ===\n",
    "import os, json, shutil, pathlib, subprocess\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "REPO_ID   = \"upstage/solar-pro-preview-instruct\"\n",
    "MODEL_DIR = pathlib.Path(\"/workspace/solar_model/model\")\n",
    "BASE      = pathlib.Path(\"/workspace\")\n",
    "\n",
    "# 0) 환경: 캐시/임시를 영구 볼륨(/workspace)로 고정 + Xet/CAS 비활성화\n",
    "TMP = BASE / \"tmp\"\n",
    "HF  = BASE / \"hf_cache\"\n",
    "for p in [TMP, HF/\"huggingface/hub\", HF/\"datasets\", HF/\"transformers\", MODEL_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"TMPDIR\"]                 = str(TMP)\n",
    "os.environ[\"HF_HOME\"]                = str(HF/\"huggingface\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"]  = str(HF/\"huggingface/hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]     = str(HF/\"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"]      = str(HF/\"datasets\")\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"]     = \"1\"   # CAS/Xet 끄기\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"  # 안정 우선(느려도 OK)\n",
    "\n",
    "# 루트 캐시에 다시 쓰지 않도록 심볼릭 링크\n",
    "subprocess.run(\"rm -rf /root/.cache/huggingface 2>/dev/null || true\", shell=True, check=False)\n",
    "subprocess.run(\"mkdir -p /root/.cache && ln -s /workspace/hf_cache/huggingface /root/.cache/huggingface\", shell=True, check=False)\n",
    "\n",
    "# 1) 실패 찌꺼기(.incomplete) 제거\n",
    "subprocess.run('find /workspace -name \"*.incomplete\" -delete', shell=True, check=False)\n",
    "\n",
    "# 2) 전체 다운로드(재개 가능, 단일 스레드)\n",
    "allow = [\n",
    "    \"config.json\",\n",
    "    \"model.safetensors.index.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"added_tokens.json\",\n",
    "    \"model-*-of-*.safetensors\",\n",
    "]\n",
    "print(\"== Download start ==\")\n",
    "local_path = snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    local_dir=str(MODEL_DIR),\n",
    "    local_dir_use_symlinks=False,   # 실제 파일 저장\n",
    "    allow_patterns=allow,\n",
    "    resume_download=True,\n",
    "    max_workers=1,                  # 단일 스레드(안정)\n",
    ")\n",
    "print(\"== Download done to:\", local_path)\n",
    "\n",
    "# 3) 무결성 검증\n",
    "index_path = MODEL_DIR / \"model.safetensors.index.json\"\n",
    "assert index_path.exists(), \"index 파일이 없습니다.\"\n",
    "index = json.loads(index_path.read_text(encoding=\"utf-8\"))\n",
    "shards = sorted(set(index.get(\"weight_map\", {}).values()))\n",
    "missing = [s for s in shards if not (MODEL_DIR / s).exists()]\n",
    "sizes   = {s: (MODEL_DIR/s).stat().st_size if (MODEL_DIR/s).exists() else 0 for s in shards}\n",
    "tiny    = [s for s, sz in sizes.items() if sz < 100*1024*1024]  # 100MB 미만 의심\n",
    "\n",
    "print(\"\\n== Verify ==\")\n",
    "print(\"총 샤드:\", len(shards))\n",
    "print(\"누락 샤드:\", missing)\n",
    "print(\"의심(너무 작은) 샤드 수:\", len(tiny))\n",
    "for s in tiny[:3]:\n",
    "    print(\" - tiny:\", s, f\"{sizes[s]/1e9:.2f} GB\")\n",
    "\n",
    "# 4) 디스크 요약\n",
    "print(\"\\n== Disk ==\")\n",
    "print(subprocess.check_output([\"df\",\"-h\",\"/workspace\"]).decode())\n",
    "\n",
    "# 5) 최종 요약\n",
    "if not missing and not tiny:\n",
    "    print(\"✅ 모든 샤드/파일 정상 다운로드로 보입니다.\")\n",
    "else:\n",
    "    print(\"⚠️ 일부 파일이 미완료/의심입니다. 이 셀을 다시 실행하면 이어받기(resume) 됩니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fabbc5-c1a0-4064-956e-4d3ad5408056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af198ba89d946d5bb06a6daaa8eb803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c4705a15a1459795bb7891435bc5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_solar.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3ef8d6eba84e988247697ec83ff7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/164 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550dd0052bf24977aa989e59e1309620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_solar.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872975c1857840728d72c546dc6853db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vllm_solar.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ code files downloaded to: /workspace/solar_model/model\n",
      "PY files: ['configuration_solar.py', 'modeling_solar.py', 'vllm_solar.py']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "REPO_ID = \"upstage/solar-pro-preview-instruct\"\n",
    "MODEL_DIR = Path(\"/workspace/solar_model/model\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 안정 옵션(느려도 OK)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "# .py 파일들과 generation_config.json만 추가로 받습니다.\n",
    "extra_allow = [\n",
    "    \"*.py\",\n",
    "    \"generation_config.json\",\n",
    "]\n",
    "path = snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    local_dir=str(MODEL_DIR),\n",
    "    local_dir_use_symlinks=False,\n",
    "    allow_patterns=extra_allow,\n",
    "    resume_download=True,\n",
    "    max_workers=1,\n",
    ")\n",
    "print(\"✅ code files downloaded to:\", path)\n",
    "\n",
    "# 확인\n",
    "py_files = sorted(p.name for p in MODEL_DIR.glob(\"*.py\"))\n",
    "print(\"PY files:\", py_files)\n",
    "assert any(\"configuration_\" in f for f in py_files), \"configuration_*.py가 아직 없어요\"\n",
    "assert any(\"modeling_\" in f for f in py_files), \"modeling_*.py가 아직 없어요\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5664d90f-b479-4338-a724-992d96ddd0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cd092190ec42de976bc06ea29f386f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로컬 모델 로드 OK\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch, os\n",
    "\n",
    "# 경고 줄이려면(선택): TRANSFORMERS_CACHE 대신 HF_HOME만 쓰도록 설정\n",
    "os.environ.pop(\"TRANSFORMERS_CACHE\", None)\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"/workspace/solar_model/model\", use_fast=False, local_files_only=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/workspace/solar_model/model\",\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,   # 이제 .py 파일이 있으니 OK\n",
    "    local_files_only=True\n",
    ")\n",
    "print(\"✅ 로컬 모델 로드 OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a98309c-51ee-4190-9055-232d08e2dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: /workspace/solar_model\n",
      "RAW : /workspace/solar_model/dataset/raw exists: True\n",
      "MODEL_DIR: /workspace/solar_model/model exists: True\n",
      "Filesystem                    Size  Used Avail Use% Mounted on\n",
      "mfs#ca-mtl-3.runpod.net:9421  420T  329T   91T  79% /workspace\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, subprocess, json\n",
    "\n",
    "BASE = pathlib.Path(\"/workspace/solar_model\")\n",
    "RAW  = BASE / \"dataset\" / \"raw\"\n",
    "PROC = BASE / \"dataset\" / \"processed\"\n",
    "OUTS = BASE / \"outputs\" / \"solar22b_qLoRA_dapt\"\n",
    "MODEL_DIR = BASE / \"model\"   # 이미 받아둔 SOLAR 22B 로컬 경로\n",
    "\n",
    "for p in [PROC, OUTS]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 캐시/임시 모두 /workspace로 고정 (컨테이너 디스크 사용 방지)\n",
    "TMP = pathlib.Path(\"/workspace/tmp\"); TMP.mkdir(parents=True, exist_ok=True)\n",
    "HF  = pathlib.Path(\"/workspace/hf_cache\")\n",
    "for p in [HF/\"huggingface/hub\", HF/\"datasets\", HF/\"transformers\"]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"TMPDIR\"] = str(TMP)\n",
    "os.environ[\"HF_HOME\"] = str(HF/\"huggingface\")\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF/\"huggingface/hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF/\"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(HF/\"datasets\")\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"  # 로컬만 사용\n",
    "\n",
    "# 루트 캐시로 새는 것 방지\n",
    "!rm -rf /root/.cache/huggingface 2>/dev/null || true\n",
    "!mkdir -p /root/.cache && ln -s /workspace/hf_cache/huggingface /root/.cache/huggingface\n",
    "\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"RAW :\", RAW, \"exists:\", RAW.exists())\n",
    "print(\"MODEL_DIR:\", MODEL_DIR, \"exists:\", MODEL_DIR.exists())\n",
    "!df -h /workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4949a676-5a64-4f41-8a73-05ce67837e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: None\n"
     ]
    }
   ],
   "source": [
    "import importlib, sys, subprocess, os\n",
    "need = []\n",
    "for m in [\"transformers\",\"datasets\",\"accelerate\",\"trl\",\"peft\",\"bitsandbytes\",\"sentencepiece\",\"pandas\",\"docx\",\"tqdm\",\"sklearn\",\"safetensors\"]:\n",
    "    try:\n",
    "        importlib.import_module(m)\n",
    "    except Exception:\n",
    "        need.append(m)\n",
    "print(\"Missing:\", need or \"None\")\n",
    "\n",
    "if need:\n",
    "    os.environ[\"PIP_CACHE_DIR\"] = \"/workspace/pip_cache\"\n",
    "    subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-U\",\n",
    "        \"transformers>=4.43.0\",\"datasets>=2.19.0\",\"accelerate>=0.33.0\",\n",
    "        \"trl>=0.9.6\",\"peft>=0.11.1\",\"bitsandbytes>=0.43.1\",\n",
    "        \"sentencepiece\",\"pandas\",\"python-docx\",\"tqdm\",\"scikit-learn\",\"safetensors\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50716ba3-257c-4f02-8a93-b4f61231822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[counts] loan: 12 notice: 3 vacancy: 3\n",
      " - /workspace/solar_model/dataset/raw/loan/1.청년전용_보증부월세대출.docx\n",
      " - /workspace/solar_model/dataset/raw/loan/10.버팀목전세자금.docx\n",
      " - /workspace/solar_model/dataset/raw/notice/서울지역본부 청년매입임대주택 예비입주자 모집공고 (1).docx\n",
      " - /workspace/solar_model/dataset/raw/notice/서울지역본부 청년매입임대주택 예비입주자 모집공고.docx\n",
      " - /workspace/solar_model/dataset/raw/vocancy/25년3차_신혼·신생아매입임대Ⅰ_공급주택목록(서울지역본부).txt\n",
      " - /workspace/solar_model/dataset/raw/vocancy/25년3차_신혼·신생아매입임대Ⅱ(전세형)_공급주택목록(서울지역본부).txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "loan_files   = sorted((RAW/\"loan\").glob(\"*.docx\"))\n",
    "notice_files = sorted((RAW/\"notice\").glob(\"*.docx\"))\n",
    "vac_files    = sorted((RAW/\"vocancy\").glob(\"*.txt\"))  # 폴더명: vocancy\n",
    "\n",
    "print(\"[counts] loan:\", len(loan_files), \"notice:\", len(notice_files), \"vacancy:\", len(vac_files))\n",
    "for p in (loan_files[:2] + notice_files[:2] + vac_files[:2]):\n",
    "    print(\" -\", p)\n",
    "assert loan_files or notice_files or vac_files, \"❌ 원시 파일을 찾지 못했습니다.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a43d268-93bd-44c0-bd51-f78cd74b7a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed files: 18\n",
      "sample: [PosixPath('/workspace/solar_model/dataset/processed/loan__1.청년전용_보증부월세대출.txt'), PosixPath('/workspace/solar_model/dataset/processed/loan__10.버팀목전세자금.txt'), PosixPath('/workspace/solar_model/dataset/processed/loan__11.갱신만료_임차인_지원_버팀목전세자금.txt')]\n"
     ]
    }
   ],
   "source": [
    "import re, os\n",
    "from docx import Document\n",
    "\n",
    "def read_docx(path):\n",
    "    doc = Document(str(path))\n",
    "    paras = []\n",
    "    for p in doc.paragraphs:\n",
    "        t = p.text.strip()\n",
    "        if t:\n",
    "            paras.append(t)\n",
    "    return \"\\n\".join(paras)\n",
    "\n",
    "def read_txt(path, encs=(\"utf-8\",\"cp949\",\"euc-kr\")):\n",
    "    for enc in encs:\n",
    "        try:\n",
    "            return Path(path).read_text(encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "ZW = re.compile(r\"[\\u200b-\\u200f\\u202a-\\u202e]\")  # zero-width 등\n",
    "def clean_text(s: str) -> str:\n",
    "    s = ZW.sub(\"\", s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n",
    "    return s\n",
    "\n",
    "def save_proc(name, text):\n",
    "    p = PROC / f\"{name}.txt\"\n",
    "    p.write_text(text, encoding=\"utf-8\")\n",
    "    return p\n",
    "\n",
    "proc_paths = []\n",
    "for p in loan_files:\n",
    "    txt = clean_text(read_docx(p))\n",
    "    proc_paths.append(save_proc(f\"loan__{p.stem}\", txt))\n",
    "for p in notice_files:\n",
    "    txt = clean_text(read_docx(p))\n",
    "    proc_paths.append(save_proc(f\"notice__{p.stem}\", txt))\n",
    "for p in vac_files:\n",
    "    txt = clean_text(read_txt(p))\n",
    "    proc_paths.append(save_proc(f\"vacancy__{p.stem}\", txt))\n",
    "\n",
    "print(\"processed files:\", len(proc_paths))\n",
    "print(\"sample:\", proc_paths[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3130b58-89ea-452c-be9c-9749d4fb6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: /workspace/solar_model/dataset/processed/dapt_train.jsonl 1267147 bytes\n",
      "val  : /workspace/solar_model/dataset/processed/dapt_val.jsonl 31967 bytes\n",
      "n_train docs: 16 n_val docs: 2\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "all_txts = sorted(PROC.glob(\"*.txt\"))\n",
    "random.shuffle(all_txts)\n",
    "\n",
    "split = int(len(all_txts) * 0.9) if len(all_txts) > 1 else len(all_txts)\n",
    "train_files = all_txts[:split]\n",
    "val_files   = all_txts[split:]\n",
    "\n",
    "def to_jsonl(files, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for fp in files:\n",
    "            text = Path(fp).read_text(encoding=\"utf-8\").strip()\n",
    "            if not text: \n",
    "                continue\n",
    "            # 문서별 헤더를 넣어 문맥 경계 명확히\n",
    "            hdr = f\"[도메인:{'대출' if 'loan__' in fp.name else '공고' if 'notice__' in fp.name else '주택목록'}] 문서명:{fp.stem}\\n\\n\"\n",
    "            rec = {\"text\": hdr + text}\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "    return out_path\n",
    "\n",
    "train_jsonl = PROC / \"dapt_train.jsonl\"\n",
    "val_jsonl   = PROC / \"dapt_val.jsonl\"\n",
    "to_jsonl(train_files, train_jsonl)\n",
    "to_jsonl(val_files, val_jsonl)\n",
    "\n",
    "print(\"train:\", train_jsonl, train_jsonl.stat().st_size, \"bytes\")\n",
    "print(\"val  :\", val_jsonl,   val_jsonl.stat().st_size, \"bytes\")\n",
    "print(\"n_train docs:\", len(train_files), \"n_val docs:\", len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed76e54-b8f4-4c0e-9821-d8182d26e3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a053a800d3b4dbe938e73f9125b61bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2b32cc6230469ea9f3655072429449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n",
      "sample train text snippet:\n",
      " [도메인:공고] 문서명:notice__서울지역본부 청년매입임대주택 예비입주자 모집공고\n",
      "\n",
      "서울지역본부 청년매입임대주택 예비입주자 모집공고\n",
      "청년 매입임대주택은 LH에서 주택을 매입하여 청년(19세~39세), 대학생 및 취업준비생을 대상으로시중시세 40~50% 수준으로 임대하는 주택입니다.\n",
      "LH에서는 마이홈센터(☏1600-1004, 내선번호 2번→3번) 및 서울지역본부 매입임대 상담센터(☏02-2015-1040)를 통해 모집공고에 대한 안내가 이루어질 수 있도록 상담을 실시하고 있습니다. (다만, 상담내용은 신청 참고자료로만 활용하여 주\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"json\", data_files={\"train\": str(train_jsonl), \"validation\": str(val_jsonl)})\n",
    "print(ds)\n",
    "print(\"sample train text snippet:\\n\", ds[\"train\"][0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70f733de-2803-49bf-948f-7c4c4b1950a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7575d952de0048a6b64dc67dc3862097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ base model ready\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # A100 OK\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(str(MODEL_DIR), use_fast=False, local_files_only=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODEL_DIR),\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "base.config.use_cache = False  # grad checkpointing 호환\n",
    "\n",
    "print(\"✅ base model ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e395502-342d-4c48-a409-ef66d57309ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 209,715,200 || all params: 22,349,747,200 || trainable%: 0.9383\n",
      "TRL available? True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository /workspace/solar_model/model contains custom code which must be executed to correctly load the model. You can inspect the repository content at /workspace/solar_model/model .\n",
      " You can inspect the repository content at https://hf.co//workspace/solar_model/model.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f2eabb92724b9c9e802fd85058f7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc492de3236b4b148973d8dced61be3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (18618 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e15061566745a2aff86b70a9a1739a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c83329b1d643abaa205942321f731b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a71e5ba7da48d89943787fb883fe2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d26794670b42089bb0a267ed225387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TRL SFTTrainer ready\n",
      "  • SFTConfig keys: ['bf16', 'eval_steps', 'eval_strategy', 'gradient_accumulation_steps', 'gradient_checkpointing', 'learning_rate', 'logging_steps', 'lr_scheduler_type', 'num_train_epochs', 'output_dir', 'per_device_eval_batch_size', 'per_device_train_batch_size', 'report_to', 'save_steps', 'save_total_limit', 'warmup_ratio']\n",
      "  • SFTTrainer keys: ['args', 'eval_dataset', 'model', 'peft_config', 'train_dataset']\n"
     ]
    }
   ],
   "source": [
    "# TRL SFTTrainer가 받는 \"키워드\"를 런타임에 자동 감지해 맞춰주고,\n",
    "# 안 맞으면 HF Trainer 파이프라인으로 자동 폴백합니다.\n",
    "\n",
    "import inspect, sys, math, torch\n",
    "from pathlib import Path\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "try:\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    HAS_TRL = True\n",
    "except Exception:\n",
    "    HAS_TRL = False\n",
    "\n",
    "# ==== 공통: LoRA 타깃(구조는 LLaMA 계열이라 아래가 정석) ====\n",
    "TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "\n",
    "# ==== QLoRA 준비 ====\n",
    "base.config.use_cache = False\n",
    "try:\n",
    "    base.gradient_checkpointing_enable()\n",
    "except Exception:\n",
    "    pass\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=TARGET_MODULES, task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "base = get_peft_model(base, lora_config)\n",
    "base.print_trainable_parameters()\n",
    "\n",
    "print(f\"TRL available? {HAS_TRL}\")\n",
    "\n",
    "# ==== TRL 경로 시도 ====\n",
    "def try_trl():\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    # SFTConfig: 버전마다 받는 인자가 다르니 필터링\n",
    "    base_cfg = dict(\n",
    "        output_dir=str(OUTS),\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=10,\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=True,           # A100이면 bf16 OK (안되면 밑에서 자동 대체)\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    sig_cfg = set(inspect.signature(SFTConfig.__init__).parameters.keys())\n",
    "    cfg = {k:v for k,v in base_cfg.items() if k in sig_cfg}\n",
    "\n",
    "    # 평가 전략 키워드 호환\n",
    "    if \"evaluation_strategy\" in sig_cfg:\n",
    "        cfg[\"evaluation_strategy\"] = \"steps\"\n",
    "    elif \"eval_strategy\" in sig_cfg:\n",
    "        cfg[\"eval_strategy\"] = \"steps\"\n",
    "    elif \"do_eval\" in sig_cfg:\n",
    "        cfg[\"do_eval\"] = True\n",
    "\n",
    "    # bf16 미지원 시 fp16 사용\n",
    "    if \"bf16\" not in sig_cfg and \"fp16\" in sig_cfg:\n",
    "        cfg[\"fp16\"] = True\n",
    "\n",
    "    train_args = SFTConfig(**cfg)\n",
    "\n",
    "    # SFTTrainer: 지원 키워드만 전달\n",
    "    sig_tr = set(inspect.signature(SFTTrainer.__init__).parameters.keys())\n",
    "    tr_kwargs = dict(model=base, args=train_args)\n",
    "\n",
    "    if \"train_dataset\" in sig_tr:  tr_kwargs[\"train_dataset\"] = ds[\"train\"]\n",
    "    if \"eval_dataset\"  in sig_tr and len(ds[\"validation\"])>0: tr_kwargs[\"eval_dataset\"] = ds[\"validation\"]\n",
    "    if \"peft_config\"   in sig_tr:  tr_kwargs[\"peft_config\"] = lora_config\n",
    "    if \"tokenizer\"     in sig_tr:  tr_kwargs[\"tokenizer\"] = tok\n",
    "    if \"dataset_text_field\" in sig_tr: tr_kwargs[\"dataset_text_field\"] = \"text\"\n",
    "    if \"packing\"       in sig_tr:  tr_kwargs[\"packing\"] = True\n",
    "    if \"max_seq_length\" in sig_tr: tr_kwargs[\"max_seq_length\"] = 4096  # Solar 2는 4096까지 OK(메모리 여유 없으면 3072/2048)\n",
    "\n",
    "    trainer = SFTTrainer(**tr_kwargs)\n",
    "    print(\"✅ TRL SFTTrainer ready\")\n",
    "    print(\"  • SFTConfig keys:\", sorted(cfg.keys()))\n",
    "    used = {k for k in tr_kwargs.keys() if k in sig_tr}\n",
    "    print(\"  • SFTTrainer keys:\", sorted(used))\n",
    "    return trainer\n",
    "\n",
    "trainer = None\n",
    "if HAS_TRL:\n",
    "    try:\n",
    "        trainer = try_trl()\n",
    "    except Exception as e:\n",
    "        print(f\"[TRL 경로 실패] -> {type(e).__name__}: {e}\")\n",
    "        trainer = None\n",
    "\n",
    "# ==== 폴백: HF Trainer (사전 토크나이즈/패킹) ====\n",
    "if trainer is None:\n",
    "    print(\"↩️  HF Trainer로 자동 폴백합니다 (사전 토크나이즈/패킹 방식).\")\n",
    "    from datasets import load_dataset\n",
    "    from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "    # 토큰화 + 패킹\n",
    "    block_size = 3072  # OOM 시 2048로 낮추세요\n",
    "    def tokenize_fn(batch):\n",
    "        return tok(batch[\"text\"], add_special_tokens=False, truncation=False)\n",
    "\n",
    "    tokenized = ds.map(tokenize_fn, batched=True, remove_columns=[c for c in ds[\"train\"].column_names if c!=\"text\"])\n",
    "    def group_texts(examples):\n",
    "        concat = []\n",
    "        for ids in examples[\"input_ids\"]:\n",
    "            concat.extend(ids)\n",
    "        total_len = (len(concat)//block_size)*block_size\n",
    "        concat = concat[:total_len]\n",
    "        chunks = [concat[i:i+block_size] for i in range(0,total_len,block_size)]\n",
    "        return {\"input_ids\": chunks, \"labels\": chunks.copy(), \"attention_mask\":[[1]*len(x) for x in chunks]}\n",
    "\n",
    "    train_tok = tokenized[\"train\"].map(group_texts, batched=True, remove_columns=[\"text\"])\n",
    "    eval_tok  = tokenized[\"validation\"].map(group_texts, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    optim_name = \"paged_adamw_8bit\"\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(OUTS),\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=True,                # 안되면 fp16=True로\n",
    "        optim=optim_name,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=base,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=eval_tok if len(eval_tok)>0 else None,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    print(\"✅ HF Trainer ready (fallback)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb4b7a99-8a2a-414f-b6c5-42afb65c706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b5f45a2eec4d93b7ef95f38a936b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository /workspace/solar_model/model contains custom code which must be executed to correctly load the model. You can inspect the repository content at /workspace/solar_model/model .\n",
      " You can inspect the repository content at https://hf.co//workspace/solar_model/model.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6bfc069bd3445d8aa1abd641ab3b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10700 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab75bbb5b2d14680af5cfd4aeb8e6b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean SFTTrainer ready (no double-LoRA)\n"
     ]
    }
   ],
   "source": [
    "# 메모리 정리\n",
    "import gc, torch\n",
    "try:\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del base\n",
    "except:\n",
    "    pass\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# ① Base(4bit)만 다시 로드 — LoRA 부착 X\n",
    "from transformers import AutoModelForCausalLM\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODEL_DIR),\n",
    "    quantization_config=bnb,         # FT-6에서 만든 BitsAndBytesConfig\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "base.config.use_cache = False\n",
    "try:\n",
    "    base.gradient_checkpointing_enable()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ② TRL SFTTrainer를 'peft_config'만 주고 생성(TRL이 알아서 LoRA 부착)\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from inspect import signature\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# SFTConfig가 허용하는 키만 적용(버전 호환)\n",
    "base_cfg = dict(\n",
    "    output_dir=str(OUTS),\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "cfg_keys = set(signature(SFTConfig.__init__).parameters.keys())\n",
    "cfg = {k:v for k,v in base_cfg.items() if k in cfg_keys}\n",
    "if \"evaluation_strategy\" in cfg_keys:\n",
    "    cfg[\"evaluation_strategy\"] = \"steps\"\n",
    "elif \"eval_strategy\" in cfg_keys:\n",
    "    cfg[\"eval_strategy\"] = \"steps\"\n",
    "elif \"do_eval\" in cfg_keys:\n",
    "    cfg[\"do_eval\"] = True\n",
    "if \"bf16\" not in cfg_keys and \"fp16\" in cfg_keys:\n",
    "    cfg[\"fp16\"] = True\n",
    "\n",
    "train_args = SFTConfig(**cfg)\n",
    "\n",
    "# SFTTrainer도 허용 키만\n",
    "tr_sig = set(signature(SFTTrainer.__init__).parameters.keys())\n",
    "tr_kwargs = dict(model=base, args=train_args, peft_config=lora_config,\n",
    "                 train_dataset=ds[\"train\"])\n",
    "if \"eval_dataset\" in tr_sig and len(ds[\"validation\"])>0:\n",
    "    tr_kwargs[\"eval_dataset\"] = ds[\"validation\"]\n",
    "# (너의 TRL 버전에선 tokenizer/dataset_text_field/max_seq_length가 인자로 없어서 자동 처리됨)\n",
    "\n",
    "trainer = SFTTrainer(**tr_kwargs)\n",
    "print(\"✅ Clean SFTTrainer ready (no double-LoRA)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77d5930c-4867-4887-8f8b-13bc6a5c8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32007}.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 학습 완료\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model(str(OUTS / \"checkpoint-last\"))\n",
    "print(\"✅ 학습 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce99cec-9f1b-4aaf-aa7f-5ac5e8a7e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA 어댑터 저장: /workspace/solar_model/outputs/solar22b_qLoRA_dapt/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "adapter_dir = OUTS / \"lora_adapter\"\n",
    "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer.model.save_pretrained(str(adapter_dir))  # LoRA 가중치\n",
    "tok.save_pretrained(str(adapter_dir))\n",
    "print(\"✅ LoRA 어댑터 저장:\", adapter_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7fa44-7ae3-492e-97ee-083a3b1c04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceff2b411e143368b62048f5eae7c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_eval = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODEL_DIR),\n",
    "    quantization_config=bnb,     # FT-6에서 만든 BitsAndBytesConfig 재사용\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "model_lora = PeftModel.from_pretrained(base_eval, str(adapter_dir))\n",
    "\n",
    "def generate(prompt, max_new_tokens=300, temperature=0.2, top_p=0.9):\n",
    "    x = tok(prompt, return_tensors=\"pt\").to(model_lora.device)\n",
    "    with torch.no_grad():\n",
    "        y = model_lora.generate(**x, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, do_sample=True)\n",
    "    print(tok.decode(y[0], skip_special_tokens=True))\n",
    "\n",
    "generate(\"‘청년전용 보증부월세대출’의 대상과 대출한도를 핵심만 요약해줘.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d736a-ec8d-450e-955f-bc9e04ea4e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
